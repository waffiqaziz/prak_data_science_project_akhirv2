---
title: "Project3"
author: "123190070 - 123190064"
date: "4/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd(dirname(rstudioapi::getSourceEditorContext()$path)) # auto set directory

# library(readr)
library(tm)
library(corpus)
library(tidymodels)
library(dplyr) # Data wrangling & manipulation
library(tidytext) # For unnest_tokens
library(stringr) # For managing text
library(ggplot2) # For data visualizations & graphs

# wordcloud
library(wordcloud)
library(RColorBrewer)

library(data.table) #library untuk mengubah dataframe to table

# library shiny
library(shiny)
```


```{r}
df <- read.csv("Coronavirus_Tweets.csv")
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- as.Date(df$TweetAt , format = "%d/%m/%y")
# df = na.omit(df)
```


melihat jumlah tiap sentiment
```{r}
p1 <- df %>%
  group_by(Sentiment) %>% 
  tally() %>% 
  ggplot(aes(x=Sentiment, y=n, fill = Sentiment)) +
  theme(axis.text.x = element_text(angle = 45, 
                                   size=14,
                                   color = "black")) +
  geom_col() +
  geom_text(
    aes(label = n), 
    colour = "black", 
    fontface = "bold",
    position=position_dodge(width=0.9), 
    vjust=-0.25)
p1
```

sentimen yang bernilai extremely positive
```{r}
df %>%
  filter(Sentiment == "Extremely Positive")
```

Jumlah Tweets dalam Satu Hari
```{r}
p2 <-df %>%
  group_by(TweetAt) %>% 
  tally() %>%
  ggplot(aes(x=TweetAt, y=n)) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_x_date(breaks = df$TweetAt) + 
  geom_col()
p2
```


show most world on location
```{r}
text <- c(df$Location)
text <- paste(text, collapse = " ")
text <- str_replace_all(text, pattern = '\"', replacement = "") # Remove slashes
text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote

text_df <- data_frame(Text = text) # tibble aka neater data frame
text_words <- text_df %>% 
  unnest_tokens(output = word, input = Text) 

# data(stop_words) # Stop words.
text_words  <- text_words  %>%
  anti_join(stop_words) # Remove stop words in peter_words
```


```{r}
# Word Counts:
text_worldcounts <- text_words  %>% count(word, sort = TRUE)
p3 <- text_worldcounts %>% 
  arrange(desc(n)) %>% 
  head(6) %>% 
  ggplot(aes(x=n,y=word))+
  geom_col() + ylab("Location") +
  theme(axis.text = element_text(size=12,
                                 color = "black"),
        legend.text = element_text(size=12))

head(text_worldcounts)
p3
```

Menghilangkan kata yang mengandung https dan @
```{r}
df$Tweets <- sapply(strsplit(as.character(df$OriginalTweet),split=' '),function(s) paste(s[!grepl('^@',s)],collapse=' ')) 

df$Tweets <- sapply(strsplit(as.character(df$Tweets),split=' '),function(s) paste(s[!grepl('^https:',s)],collapse=' '))


df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:@?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)

# coba gunakan peritnh dibawah jika masih terdapat kata yang tidak perlu
# df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:https?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)

# df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:amp?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)
# df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:[\r\n]?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)
# df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:[[:punct:]]?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)
```

world cloud
```{r}
docs <- Corpus(VectorSource(df$Tweets))

# pre-processing
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>% #Remove commas etc.
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument) # menghilangkan kata mirip (crack, cracked, cracking)
docs <- tm_map(docs, content_transformer(tolower))

# menghilangkan konjungsi
docs <- tm_map(docs, removeWords, stopwords("english"))
```
```{r}
# set.seed(1234) # for reproducibility 
wordcloud1 <- wordcloud(docs
                       , max.words=500    # Set top n words
                       , random.order=FALSE # Words in decreasing freq
                       , rot.per=0.35       # % of vertical words
                       , use.r.layout=FALSE # Use C++ collision detection
                       , colors=brewer.pal(8, "Dark2"))
```



```{r}
words <- df$Tweets

words <- data_frame(Text = words)
# head(words, n = 20)
words <- words %>%
  # tm_map(removePunctuation) %>%
  unnest_tokens(output = word, input = Text)%>%
  anti_join(stop_words) %>% # Remove stop words in peter_words
  count(word, sort = TRUE)

words = na.omit(words) #hapus baris yang kososng


# words <- gsub("[0-9]+", "", words$word) 
```

Plot top words
```{r}
p4 <- words %>% 
  filter(n > 2000) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() +
  coord_flip() +
  labs(x = "Word \n", y = "\n Count ", title = "Frequent Words\n") +
  geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="darkblue", size = 12),
        axis.title.y = element_text(face="bold", colour="darkblue", size = 12))
p4
```

unsupervised
```{r}
unsupervised <- df %>% 
  filter(!is.na(Tweets)) %>% 
  group_by(Sentiment) %>% 
  unnest_tokens(word,Tweets) %>% 
  ungroup() %>% 
  anti_join(stop_words)
unsupervised
```
```{r}
unsupervised %>% 
  count(Sentiment,word,sort = TRUE)
```

rf-idf
```{r}
# unsupervised <- unsupervised %>% 
#   count(Sentiment,word,sort = TRUE) %>% 
#   bind_tf_idf(word, Sentiment,n)
# unsupervised
```



```{r}
covid_dtm <- unsupervised %>% 
  count(Sentiment,word) %>% 
  cast_dtm(Sentiment,word,n)
covid_dtm
```
```{r}
# covid_lda <- LDA(covid_dtm, k=5)
# covid_lda
```

SHINY
```{r}
ui <- shinyUI(
  navbarPage("Analisis Sentimen",
             tabPanel("Jumlah Sentimet",
                      mainPanel(
                        h2("Jumlah Tiap Sentiment", align = "center"),
                        plotOutput(outputId = "plot1"),width = "100%")
             ),
             tabPanel("Tweets Harian",
                      mainPanel(
                        h2("Jumlah Tweets dalam Satu Hari", align = "center"),
                        plotOutput(outputId = "plot2"),width = "100%")
             ),
             tabPanel("Lokasi Tweets",
                      mainPanel(
                        h2("Jumlah Tweets Terbanyak Berdasarkan Lokasi", 
                           align = "center"),
                        plotOutput(outputId = "plot3"),width = "100%")
             ),
             tabPanel("Jumlah Kata",
                      mainPanel(
                        h2("Jumlah Masing-Masing Kata Pada Tweets", 
                           align = "center"),
                        plotOutput(outputId = "plot4"),width = "100%")
             ),
             tabPanel("World Cloud",
                      mainPanel(
                        h2("World Cloud Covid-19 Tweets", 
                           align = "center"),
                        plotOutput(outputId = "worldcloud"),width = "100%")
             )
  )
)

server <- function(input, output) {
  output$plot1 <-renderPlot({
    plot(p1)
  })
  output$plot2 <-renderPlot({
    plot(p2)
  })
  output$plot3 <-renderPlot({
    plot(p3)
  })
  output$plot4 <-renderPlot({
    plot(p4)
  })
  output$worldcloud <-renderPlot({
    wordcloud(docs
                       , max.words=500    # Set top n words
                       , random.order=FALSE # Words in decreasing freq
                       , rot.per=0.35       # % of vertical words
                       , use.r.layout=FALSE # Use C++ collision detection
                       , colors=brewer.pal(8, "Dark2"))
  })
  
  # output$table1 <- renderTable({
  #   table
  #   table(df %>% head(10)
  #         )
  # })
  output$table1 <- renderTable(df)
}

shinyApp(ui, server)
```


